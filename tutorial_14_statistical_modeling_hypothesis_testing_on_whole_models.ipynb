{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Title: Hypothesis Testing on Whole Models\n",
      "Date: 2013-10-08 17:05\n",
      "Author: cfarmer\n",
      "Email: carson.farmer@gmail.com\n",
      "Category: Statistical Modeling for Python\n",
      "Tags: Helpful tips, Python, Statistical Modeling, Teaching\n",
      "Slug: statistical-modeling-python-hypothesis-testing-whole\n",
      "Latex: yes\n",
      "Status: draft"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Hypothesis Testing on Whole Models\n",
      "\n",
      "This tutorial presents two ways of doing hypothesis tests on whole models: 1) permutation tests where the connection is severed between the explanatory and response variables, and 2) tests such as ANOVA where the sampling distribution is calculated from first principals. In practice, first-principle tests are used most of the time. Still, the permutation test is useful for developing intuition about hypothesis testing - our main purpose here - and for those not-so-rare occaisons where the assumptions behind the first-principle tests as dubious."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Permutation Test\n",
      "\n",
      "The idea of a permutation test is to enfore the null hypothesis that there is no connection between the response variables and the explanatory variables. An effective way to do this is to randomize the response variable in a way that is consistent with sampling variability. When constructing confidence intervals, re-sampling was used. Re-sampling will typically repeat some cases and omit others. Here, we are more interested in scrambling the order of one or more variables while leaving the others in their original state. The `permutation` function from the `numpy.random` module will be used to accomplish this.\n",
      "\n",
      "To illustrate, consider a model for exploring whether `sex` and `mother`'s height are related to the height of the child:\n",
      "\n",
      "<span class=\"dataset shadow\"><i class=\"icon-flag\" style=\"font-size: 1.5em;\"></i> `galton.csv`</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import statsmodels.formula.api as sm\n",
      "import pandas as pd\n",
      "\n",
      "galton = pd.read_csv(\"http://www.mosaic-web.org/go/datasets/galton.csv\")\n",
      "mod = sm.ols(\"height ~ sex + mother\", data=galton)\n",
      "fit = mod.fit()\n",
      "print fit.params, \"\\nrsquared     \", fit.rsquared # Print out the rsquared value nicely"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Intercept    41.449524\n",
        "sex[T.M]      5.176695\n",
        "mother        0.353137\n",
        "dtype: float64 \n",
        "rsquared      0.561801885474\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parameters indicate that typical males are taller than typical females by about 5 inches and that for each inch taller the mother is, a child will typically be taller by 0.35 inches. A reasonable test statistic to summarize the whole model is $R^2$. The summary report shows this to be $R^2 = 0.562$ (not shown).\n",
      "\n",
      "For confidence intervals, re-sampling was applied to the entire data frame. This selects random cases, but each selected case is an authentic one that matches exactly the original values for that case. The point of re-sampling is to get an idea of the variability introduced by random sampling of authentic cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "def resample(df, n=None, replace=True):\n",
      "    if n is None:\n",
      "        n = df.shape[0]\n",
      "    choices = np.random.choice(df.index, n, replace=replace)\n",
      "    return df.ix[choices]\n",
      "\n",
      "pd.DataFrame([sm.ols(\"height ~ sex + mother\", data=resample(galton)).fit().params for i in range(5)], \n",
      "             index=range(5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Intercept</th>\n",
        "      <th>sex[T.M]</th>\n",
        "      <th>mother</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 42.757400</td>\n",
        "      <td> 4.958629</td>\n",
        "      <td> 0.334337</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 44.088196</td>\n",
        "      <td> 5.040707</td>\n",
        "      <td> 0.315130</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 41.784323</td>\n",
        "      <td> 5.290200</td>\n",
        "      <td> 0.346561</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 38.903076</td>\n",
        "      <td> 5.377676</td>\n",
        "      <td> 0.392889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 41.844829</td>\n",
        "      <td> 5.070087</td>\n",
        "      <td> 0.348238</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "   Intercept  sex[T.M]    mother\n",
        "0  42.757400  4.958629  0.334337\n",
        "1  44.088196  5.040707  0.315130\n",
        "2  41.784323  5.290200  0.346561\n",
        "3  38.903076  5.377676  0.392889\n",
        "4  41.844829  5.070087  0.348238"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `sex[T.M]` parameters are tightly grouped hear 5 inches, the `mother` parameters are around 0.3 to 0.4.\n",
      "\n",
      "In order to carry out a permutation test, do not randomize the whole data frame. Instead, shuffle just the response variable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import permutation\n",
      "pd.DataFrame([sm.ols(\"permutation(height) ~ sex + mother\", data=galton).fit().params for i in range(5)], \n",
      "             index=range(5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Intercept</th>\n",
        "      <th>sex[T.M]</th>\n",
        "      <th>mother</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 70.095853</td>\n",
        "      <td> 0.215405</td>\n",
        "      <td>-0.053784</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 71.079129</td>\n",
        "      <td> 0.576786</td>\n",
        "      <td>-0.072047</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 63.128318</td>\n",
        "      <td>-0.013061</td>\n",
        "      <td> 0.056787</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 65.897598</td>\n",
        "      <td> 0.459107</td>\n",
        "      <td> 0.009758</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 68.360830</td>\n",
        "      <td>-0.186364</td>\n",
        "      <td>-0.023463</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "   Intercept  sex[T.M]    mother\n",
        "0  70.095853  0.215405 -0.053784\n",
        "1  71.079129  0.576786 -0.072047\n",
        "2  63.128318 -0.013061  0.056787\n",
        "3  65.897598  0.459107  0.009758\n",
        "4  68.360830 -0.186364 -0.023463"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the `sex[T.M]` and `mother` parameters are close to zero, as would be expected when there is no relationship betweem the response variable and the explanatory variables.\n",
      "\n",
      "In constructing the sampling distribution under the null hypothesis, you should do hundreds of trials of fitting the model to the scrambled data, calculating the test statistic (we'll use $R^2$ here) for each trial. Note that each trial itself involves all of the cases in your sample, but those cases have been changed so that the shuffled (or permuted) variable almost certainly takes on a different value in every case than in the original data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nulls = pd.Series([sm.ols(\"permutation(height) ~ sex + mother\", data=galton).fit().rsquared for i in range(500)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above produces a Pandas Series containing 500 $R^2$ values from the permutation test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nulls.head() # There are 500 cases all together"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0    0.000741\n",
        "1    0.000326\n",
        "2    0.000777\n",
        "3    0.004096\n",
        "4    0.000761\n",
        "dtype: float64"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naturally, all of the $R^2$ values for the trials are close to zero. After all, there is no relation between the response variable (after randomization with `permutation()`) and the explanatory variables.\n",
      "\n",
      "The p-value can be calculated directly from the trials, by comparison to the observed value in the actual data: $R^2$ was 0.5618."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(nulls>0.5618).value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "False    500\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "None of the 500 trials were greater than the value of the test statistic, 0.5618 (hence the count of 500 False's). It wouldn't be fair to claim that $p = 0$, since we only did 500 trials, but it is reasonable to say that the permutation test shows the p-value is $p < 1/500$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## First-Principle Tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On modern computers, the permutation test is entirely practical. But a few decades ago, it was not. Great creativity was applied to finding test statistics where the sampling distribution could be estimated without extensive calculation. One of these is the F statistic. This is still very useful today and is a standard part of the regression report in many statistical packages.\n",
      "\n",
      "Here is the regression report from the `height ~ sex + mother` model design presented earlier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod = sm.ols(\"height ~ sex + mother\", data=galton)\n",
      "fit = mod.fit()\n",
      "fit.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table class=\"simpletable\">\n",
        "<caption>OLS Regression Results</caption>\n",
        "<tr>\n",
        "  <th>Dep. Variable:</th>         <td>height</td>      <th>  R-squared:         </th> <td>   0.562</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.561</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   573.7</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Date:</th>             <td>Tue, 08 Oct 2013</td> <th>  Prob (F-statistic):</th> <td>4.44e-161</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Time:</th>                 <td>21:58:23</td>     <th>  Log-Likelihood:    </th> <td> -2049.3</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>No. Observations:</th>      <td>   898</td>      <th>  AIC:               </th> <td>   4105.</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Df Residuals:</th>          <td>   895</td>      <th>  BIC:               </th> <td>   4119.</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Intercept</th> <td>   41.4495</td> <td>    2.209</td> <td>   18.760</td> <td> 0.000</td> <td>   37.113    45.786</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>sex[T.M]</th>  <td>    5.1767</td> <td>    0.159</td> <td>   32.625</td> <td> 0.000</td> <td>    4.865     5.488</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>mother</th>    <td>    0.3531</td> <td>    0.034</td> <td>   10.270</td> <td> 0.000</td> <td>    0.286     0.421</td>\n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "  <th>Omnibus:</th>       <td> 4.661</td> <th>  Durbin-Watson:     </th> <td>   1.290</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Prob(Omnibus):</th> <td> 0.097</td> <th>  Jarque-Bera (JB):  </th> <td>   4.977</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Skew:</th>          <td>-0.103</td> <th>  Prob(JB):          </th> <td>  0.0830</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Kurtosis:</th>      <td> 3.301</td> <th>  Cond. No.          </th> <td>1.79e+03</td>\n",
        "</tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "<class 'statsmodels.iolib.summary.Summary'>\n",
        "\"\"\"\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                 height   R-squared:                       0.562\n",
        "Model:                            OLS   Adj. R-squared:                  0.561\n",
        "Method:                 Least Squares   F-statistic:                     573.7\n",
        "Date:                Tue, 08 Oct 2013   Prob (F-statistic):          4.44e-161\n",
        "Time:                        21:58:23   Log-Likelihood:                -2049.3\n",
        "No. Observations:                 898   AIC:                             4105.\n",
        "Df Residuals:                     895   BIC:                             4119.\n",
        "Df Model:                           2                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept     41.4495      2.209     18.760      0.000        37.113    45.786\n",
        "sex[T.M]       5.1767      0.159     32.625      0.000         4.865     5.488\n",
        "mother         0.3531      0.034     10.270      0.000         0.286     0.421\n",
        "==============================================================================\n",
        "Omnibus:                        4.661   Durbin-Watson:                   1.290\n",
        "Prob(Omnibus):                  0.097   Jarque-Bera (JB):                4.977\n",
        "Skew:                          -0.103   Prob(JB):                       0.0830\n",
        "Kurtosis:                       3.301   Cond. No.                     1.79e+03\n",
        "==============================================================================\n",
        "\n",
        "Warnings:\n",
        "[1] The condition number is large, 1.79e+03. This might indicate that there are\n",
        "strong multicollinearity or other numerical problems.\n",
        "\"\"\""
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The third row of the first table in the report shows an F statistic of 573.7 based on an $R^2$ of 0.562 and translates this to a p-value (`Prob (F-statistic)`) that is practically zero: `4.44e-161`.\n",
      "\n",
      "By way of showing that the regression report is rooted in the same approach show in Chapter 14 of ['Statistical Modeling: A Fresh Approach (2nd Edition)'][book], you can confirm the calculations. There are $m = 3$ parameters and $n = 898$ cases, producing $n - m = 895$ degrees of freedom in the denominator and $m - 1 = 2$ degrees of freedom in the numerator. The calculation of the F statistic from $R^2$ and the degrees of freedom follows the formula given in the chapter:\n",
      "\n",
      "$$F = \\frac{R^2}{m - 1} \\Big / \\frac{1 - R^2}{n - m}$$\n",
      "\n",
      "Plugging the values into the formula:\n",
      "\n",
      "[book]: http://www.mosaic-web.org/go/StatisticalModeling/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(0.562/2.) / ((1.-0.562)/895.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "574.1894977168951"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F is the test statistic. To convert it to a p-value, you need to calculate how extreme the value of F = 574.19 is with reference to the F distribution with 895 and 2 degrees of freedom."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import f\n",
      "1. - f.cdf(574.189, 2, 895)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "1.1102230246251565e-16"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The calculation of p-values from F always follows this form. In the context of the F distribution, \"extreme\" always means \"bigger than\". So, calculate the area under the F distribution to the right of the observed value."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Next time on, Statistical Modeling: A Fresh Approach for Python...\n",
      "\n",
      "* **Hypothesis Testing on Parts of Models**\n",
      "\n",
      "### Reference\n",
      "\n",
      "As with all 'Statistical Modeling: A Fresh Approach for Python' tutorials, this tutorial is based directly on material from ['Statistical Modeling: A Fresh Approach (2nd Edition)'][book] by [Daniel Kaplan][daniel-kaplan].\n",
      "\n",
      "I have made an effort to keep the text and explanations consistent between the original (R-based) version and the Python tutorials, in order to keep things comparable. With that in mind, any errors, omissions, and/or differences between the two versions are mine, and any questions, comments, and/or concerns should be [directed to me][].\n",
      "\n",
      "[book]: http://www.mosaic-web.org/go/StatisticalModeling/\n",
      "[daniel-kaplan]: http://www.macalester.edu/~kaplan/\n",
      "[directed to me]: mailto:carson.farmer@hunter.cuny.edu"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}